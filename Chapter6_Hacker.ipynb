{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el8l05WQEO46"
      },
      "source": [
        "# **Line search**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "xhmIOLiZELV_"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from matplotlib.colors import ListedColormap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "qFRe9POHF2le"
      },
      "outputs": [],
      "source": [
        "# Creatinng simple 1D function\n",
        "def loss_function(phi):\n",
        "  return 1- 0.4 * np.exp(-(phi-0.55)*(phi-0.35)/0.08) - 0.35 *np.exp(-(phi-0.28)*(phi-0.44)/0.1)\n",
        "\n",
        "def draw_function(loss_function,a=None, b=None, c=None, d=None):\n",
        "  # Plot the function\n",
        "  phi_plot = np.arange(0,1,0.01);\n",
        "  fig,ax = plt.subplots()\n",
        "  ax.plot(phi_plot,loss_function(phi_plot),'r-')\n",
        "  ax.set_xlim(0,1); ax.set_ylim(0,1)\n",
        "  ax.set_xlabel(r'$\\phi$'); ax.set_ylabel(r'$L[\\phi]$')\n",
        "  if a is not None and b is not None and c is not None and d is not None:\n",
        "      plt.axvspan(a, d, facecolor='k', alpha=0.2)\n",
        "      ax.plot([a,a],[0,1],'b-')\n",
        "      ax.plot([b,b],[0,1],'b-')\n",
        "      ax.plot([c,c],[0,1],'b-')\n",
        "      ax.plot([d,d],[0,1],'b-')\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXx1Tpd1Tl-I"
      },
      "outputs": [],
      "source": [
        "# Draw this function\n",
        "draw_function(loss_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU5mdGvpTtEG"
      },
      "source": [
        "Now we use line search to find the minimum in the range 0,1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "id": "K-NTHpAAHlCl"
      },
      "outputs": [],
      "source": [
        "def line_search(loss_function, thresh=.0001, max_iter = 10, draw_flag = False):\n",
        "\n",
        "    # Initialize four points along the range we are going to search\n",
        "    a = 0\n",
        "    b = 0.25\n",
        "    c = 0.75\n",
        "    d = 1.0\n",
        "    n_iter = 0\n",
        "\n",
        "    # While we haven't found the minimum closely enough\n",
        "    while np.abs(b-c) > thresh and n_iter < max_iter:\n",
        "        # Increment iteration counter (just to prevent an infinite loop)\n",
        "        n_iter = n_iter+1\n",
        "\n",
        "        # Calculate all four points\n",
        "        lossa = loss_function(a)\n",
        "        lossb = loss_function(b)\n",
        "        lossc = loss_function(c)\n",
        "        lossd = loss_function(d)\n",
        "\n",
        "        if draw_flag:\n",
        "          draw_function(loss_function, a,b,c,d)\n",
        "\n",
        "        print('Iter %d, a=%3.3f, b=%3.3f, c=%3.3f, d=%3.3f'%(n_iter, a,b,c,d))\n",
        "\n",
        "        # Rule #1 If the HEIGHT at point A is less than the HEIGHT at points B, C, and D then move them so they are half\n",
        "        # as far from A as they start\n",
        "        if lossa < lossb and lossa < lossc and lossa < lossd:\n",
        "            b = a + (b - a) / 2\n",
        "            c = a + (c - a) / 2\n",
        "            d = a + (d - a) / 2\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Rule #2 If the HEIGHT at point b is less than the HEIGHT at point c then\n",
        "        #                     point d becomes point c, and\n",
        "        #                     point b becomes 1/3 between a and new d\n",
        "        #                     point c becomes 2/3 between a and new d\n",
        "        if lossb < lossc:\n",
        "            d = c\n",
        "            b = a + (d - a) / 3\n",
        "            c = a + 2 * (d - a) / 3\n",
        "            continue\n",
        "\n",
        "        # Rule #3 If the HEIGHT at point c is less than the HEIGHT at point b then\n",
        "        #                     point a becomes point b, and\n",
        "        #                     point b becomes 1/3 between new a and d\n",
        "        #                     point c becomes 2/3 between new a and d\n",
        "        if lossc < lossb:\n",
        "            a = b\n",
        "            b = a + (d - a) / 3\n",
        "            c = a + 2 * (d - a) / 3\n",
        "            continue\n",
        "\n",
        "    soln = (b + c) / 2\n",
        "\n",
        "\n",
        "    return soln"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVq6rmaWRD2M"
      },
      "outputs": [],
      "source": [
        "soln = line_search(loss_function, draw_flag=True)\n",
        "print('Soln = %3.3f, loss = %3.3f'%(soln,loss_function(soln)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF_6avC0QpEp"
      },
      "source": [
        "# **Gradient descent**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "4cRkrh9MZ58Z"
      },
      "outputs": [],
      "source": [
        "# Create training data 12 pairs {x_i, y_i}\n",
        "# We'll try to fit the straight line model to these data\n",
        "data = np.array([[0.03,0.15,0.33,0.46,0.78,0.81,1.08,1.18,1.39,1.60,1.65,1.90],\n",
        "                 [0.67,0.55,1.05,1.00,1.40,1.40,1.30,1.54,1.55,1.38,1.73,1.60]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "id": "WQUERmb2erAe"
      },
      "outputs": [],
      "source": [
        "# Let's define our model -- just a straight line with intercept phi[0] and slope phi[1]\n",
        "def model(phi,x):\n",
        "  y_pred = phi[0]+phi[1] * x\n",
        "  return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "zWP8BdqGQpEs"
      },
      "outputs": [],
      "source": [
        "# Draw model\n",
        "def draw_model(data,model,phi,title=None):\n",
        "  x_model = np.arange(0,2,0.01)\n",
        "  y_model = model(phi,x_model)\n",
        "\n",
        "  fix, ax = plt.subplots()\n",
        "  ax.plot(data[0,:],data[1,:],'bo')\n",
        "  ax.plot(x_model,y_model,'m-')\n",
        "  ax.set_xlim([0,2]);ax.set_ylim([0,2])\n",
        "  ax.set_xlabel('x'); ax.set_ylabel('y')\n",
        "  ax.set_aspect('equal')\n",
        "  if title is not None:\n",
        "    ax.set_title(title)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FeK1KqoQpEs"
      },
      "outputs": [],
      "source": [
        "# Initialize the parameters to some arbitrary values and draw the model\n",
        "phi = np.zeros((2,1))\n",
        "phi[0] = 0.3     # Intercept\n",
        "phi[1] = -0.1      # Slope\n",
        "draw_model(data,model,phi, \"Initial parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ommFQHmIQpEs"
      },
      "source": [
        "Now let's compute the sum of squares loss for the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "id": "I7dqTY2Gg7CR"
      },
      "outputs": [],
      "source": [
        "def compute_loss(data_x, data_y, model, phi):\n",
        "  # Compute model predictions from data_x\n",
        "    pred_y = model(phi, data_x)\n",
        "\n",
        "    # Compute the squared differences between predictions and true y values\n",
        "    squared_diffs = (pred_y - data_y) ** 2\n",
        "\n",
        "    # Sum the squared differences\n",
        "    loss = np.sum(squared_diffs)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3trnavPiHpH"
      },
      "source": [
        "Now let's plot the whole loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "id": "k8NI4a7eQpEt"
      },
      "outputs": [],
      "source": [
        "def draw_loss_function(compute_loss, data,  model, phi_iters = None):\n",
        "  # Define pretty colormap\n",
        "  my_colormap_vals_hex =('2a0902', '2b0a03', '2c0b04', '2d0c05', '2e0c06', '2f0d07', '300d08', '310e09', '320f0a', '330f0b', '34100b', '35110c', '36110d', '37120e', '38120f', '39130f', '3a1410', '3b1411', '3c1511', '3d1612', '3e1613', '3f1713', '401714', '411814', '421915', '431915', '451a16', '461b16', '471b17', '481c17', '491d18', '4a1d18', '4b1e19', '4c1f19', '4d1f1a', '4e201b', '50211b', '51211c', '52221c', '53231d', '54231d', '55241e', '56251e', '57261f', '58261f', '592720', '5b2821', '5c2821', '5d2922', '5e2a22', '5f2b23', '602b23', '612c24', '622d25', '632e25', '652e26', '662f26', '673027', '683027', '693128', '6a3229', '6b3329', '6c342a', '6d342a', '6f352b', '70362c', '71372c', '72372d', '73382e', '74392e', '753a2f', '763a2f', '773b30', '783c31', '7a3d31', '7b3e32', '7c3e33', '7d3f33', '7e4034', '7f4134', '804235', '814236', '824336', '834437', '854538', '864638', '874739', '88473a', '89483a', '8a493b', '8b4a3c', '8c4b3c', '8d4c3d', '8e4c3e', '8f4d3f', '904e3f', '924f40', '935041', '945141', '955242', '965343', '975343', '985444', '995545', '9a5646', '9b5746', '9c5847', '9d5948', '9e5a49', '9f5a49', 'a05b4a', 'a15c4b', 'a35d4b', 'a45e4c', 'a55f4d', 'a6604e', 'a7614e', 'a8624f', 'a96350', 'aa6451', 'ab6552', 'ac6552', 'ad6653', 'ae6754', 'af6855', 'b06955', 'b16a56', 'b26b57', 'b36c58', 'b46d59', 'b56e59', 'b66f5a', 'b7705b', 'b8715c', 'b9725d', 'ba735d', 'bb745e', 'bc755f', 'bd7660', 'be7761', 'bf7862', 'c07962', 'c17a63', 'c27b64', 'c27c65', 'c37d66', 'c47e67', 'c57f68', 'c68068', 'c78169', 'c8826a', 'c9836b', 'ca846c', 'cb856d', 'cc866e', 'cd876f', 'ce886f', 'ce8970', 'cf8a71', 'd08b72', 'd18c73', 'd28d74', 'd38e75', 'd48f76', 'd59077', 'd59178', 'd69279', 'd7937a', 'd8957b', 'd9967b', 'da977c', 'da987d', 'db997e', 'dc9a7f', 'dd9b80', 'de9c81', 'de9d82', 'df9e83', 'e09f84', 'e1a185', 'e2a286', 'e2a387', 'e3a488', 'e4a589', 'e5a68a', 'e5a78b', 'e6a88c', 'e7aa8d', 'e7ab8e', 'e8ac8f', 'e9ad90', 'eaae91', 'eaaf92', 'ebb093', 'ecb295', 'ecb396', 'edb497', 'eeb598', 'eeb699', 'efb79a', 'efb99b', 'f0ba9c', 'f1bb9d', 'f1bc9e', 'f2bd9f', 'f2bfa1', 'f3c0a2', 'f3c1a3', 'f4c2a4', 'f5c3a5', 'f5c5a6', 'f6c6a7', 'f6c7a8', 'f7c8aa', 'f7c9ab', 'f8cbac', 'f8ccad', 'f8cdae', 'f9ceb0', 'f9d0b1', 'fad1b2', 'fad2b3', 'fbd3b4', 'fbd5b6', 'fbd6b7', 'fcd7b8', 'fcd8b9', 'fcdaba', 'fddbbc', 'fddcbd', 'fddebe', 'fddfbf', 'fee0c1', 'fee1c2', 'fee3c3', 'fee4c5', 'ffe5c6', 'ffe7c7', 'ffe8c9', 'ffe9ca', 'ffebcb', 'ffeccd', 'ffedce', 'ffefcf', 'fff0d1', 'fff2d2', 'fff3d3', 'fff4d5', 'fff6d6', 'fff7d8', 'fff8d9', 'fffada', 'fffbdc', 'fffcdd', 'fffedf', 'ffffe0')\n",
        "  my_colormap_vals_dec = np.array([int(element,base=16) for element in my_colormap_vals_hex])\n",
        "  r = np.floor(my_colormap_vals_dec/(256*256))\n",
        "  g = np.floor((my_colormap_vals_dec - r *256 *256)/256)\n",
        "  b = np.floor(my_colormap_vals_dec - r * 256 *256 - g * 256)\n",
        "  my_colormap = ListedColormap(np.vstack((r,g,b)).transpose()/255.0)\n",
        "\n",
        "  # Make grid of intercept/slope values to plot\n",
        "  intercepts_mesh, slopes_mesh = np.meshgrid(np.arange(0.0,2.0,0.02), np.arange(-1.0,1.0,0.002))\n",
        "  loss_mesh = np.zeros_like(slopes_mesh)\n",
        "  # Compute loss for every set of parameters\n",
        "  for idslope, slope in np.ndenumerate(slopes_mesh):\n",
        "     loss_mesh[idslope] = compute_loss(data[0,:], data[1,:], model, np.array([[intercepts_mesh[idslope]], [slope]]))\n",
        "\n",
        "  fig,ax = plt.subplots()\n",
        "  fig.set_size_inches(8,8)\n",
        "  ax.contourf(intercepts_mesh,slopes_mesh,loss_mesh,256,cmap=my_colormap)\n",
        "  ax.contour(intercepts_mesh,slopes_mesh,loss_mesh,40,colors=['#80808080'])\n",
        "  if phi_iters is not None:\n",
        "    ax.plot(phi_iters[0,:], phi_iters[1,:],'go-')\n",
        "  ax.set_ylim([1,-1])\n",
        "  ax.set_xlabel('Intercept $\\phi_{0}$'); ax.set_ylabel('Slope, $\\phi_{1}$')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8HbvIupnTME"
      },
      "outputs": [],
      "source": [
        "draw_loss_function(compute_loss, data, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9Duf05WqqSC"
      },
      "source": [
        "Compute the gradient vector for a given set of parameters:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial L}{\\partial \\boldsymbol\\phi} = \\begin{bmatrix}\\frac{\\partial L}{\\partial \\phi_0} \\\\\\frac{\\partial L}{\\partial \\phi_1} \\end{bmatrix}.\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "UpswmkL2qwBT"
      },
      "outputs": [],
      "source": [
        "def compute_gradient(data_x, data_y, phi):\n",
        "    phi0, phi1 = phi[0, 0], phi[1, 0]\n",
        "\n",
        "    # Predictions using current phi\n",
        "    pred_y = phi0 + phi1 * data_x\n",
        "\n",
        "    # Errors\n",
        "    error = pred_y - data_y\n",
        "\n",
        "    # Compute gradients\n",
        "    dl_dphi0 = 2 * np.sum(error)\n",
        "    dl_dphi1 = 2 * np.sum(error * data_x)\n",
        "\n",
        "    # Return the gradient\n",
        "    return np.array([[dl_dphi0],[dl_dphi1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuwAHN7yt-gi"
      },
      "outputs": [],
      "source": [
        "# Compute the gradient using your function\n",
        "gradient = compute_gradient(data[0,:],data[1,:], phi)\n",
        "print(\"Your gradients: (%3.3f,%3.3f)\"%(gradient[0],gradient[1]))\n",
        "# Approximate the gradients with finite differences\n",
        "delta = 0.0001\n",
        "dl_dphi0_est = (compute_loss(data[0,:],data[1,:],model,phi+np.array([[delta],[0]])) - \\\n",
        "                    compute_loss(data[0,:],data[1,:],model,phi))/delta\n",
        "dl_dphi1_est = (compute_loss(data[0,:],data[1,:],model,phi+np.array([[0],[delta]])) - \\\n",
        "                    compute_loss(data[0,:],data[1,:],model,phi))/delta\n",
        "print(\"Approx gradients: (%3.3f,%3.3f)\"%(dl_dphi0_est,dl_dphi1_est))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EIjMM9Fw2eT"
      },
      "source": [
        "Now we are ready to perform gradient descent.  We'll need to use line search (implemented more generally) plus the helper function loss_function_1D that maps the search along the negative gradient direction in 2D space to a 1D problem (distance along this direction)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "id": "XrJ2gQjfw1XP"
      },
      "outputs": [],
      "source": [
        "def loss_function_1D(dist_prop, data, model, phi_start, search_direction):\n",
        "  # Return the loss after moving this far\n",
        "  return compute_loss(data[0,:], data[1,:], model, phi_start+ search_direction * dist_prop)\n",
        "\n",
        "def line_search(data, model, phi, gradient, thresh=.00001, max_dist = 0.1, max_iter = 15, verbose=False):\n",
        "    # Initialize four points along the range we are going to search\n",
        "    a = 0\n",
        "    b = 0.33 * max_dist\n",
        "    c = 0.66 * max_dist\n",
        "    d = 1.0 * max_dist\n",
        "    n_iter = 0\n",
        "\n",
        "    # While we haven't found the minimum closely enough\n",
        "    while np.abs(b-c) > thresh and n_iter < max_iter:\n",
        "        # Increment iteration counter (just to prevent an infinite loop)\n",
        "        n_iter = n_iter+1\n",
        "        # Calculate all four points\n",
        "        lossa = loss_function_1D(a, data, model, phi,gradient)\n",
        "        lossb = loss_function_1D(b, data, model, phi,gradient)\n",
        "        lossc = loss_function_1D(c, data, model, phi,gradient)\n",
        "        lossd = loss_function_1D(d, data, model, phi,gradient)\n",
        "\n",
        "        if verbose:\n",
        "          print('Iter %d, a=%3.3f, b=%3.3f, c=%3.3f, d=%3.3f'%(n_iter, a,b,c,d))\n",
        "          print('a %f, b%f, c%f, d%f'%(lossa,lossb,lossc,lossd))\n",
        "\n",
        "        # Rule #1 If point A is less than points B, C, and D then halve distance from A to points B,C, and D\n",
        "        if np.argmin((lossa,lossb,lossc,lossd))==0:\n",
        "          b = a+ (b-a)/2\n",
        "          c = a+ (c-a)/2\n",
        "          d = a+ (d-a)/2\n",
        "          continue;\n",
        "\n",
        "        # Rule #2 If point b is less than point c then\n",
        "        #                     point d becomes point c, and\n",
        "        #                     point b becomes 1/3 between a and new d\n",
        "        #                     point c becomes 2/3 between a and new d\n",
        "        if lossb < lossc:\n",
        "          d = c\n",
        "          b = a+ (d-a)/3\n",
        "          c = a+ 2*(d-a)/3\n",
        "          continue\n",
        "\n",
        "        # Rule #2 If point c is less than point b then\n",
        "        #                     point a becomes point b, and\n",
        "        #                     point b becomes 1/3 between new a and d\n",
        "        #                     point c becomes 2/3 between new a and d\n",
        "        a = b\n",
        "        b = a+ (d-a)/3\n",
        "        c = a+ 2*(d-a)/3\n",
        "\n",
        "    # Return average of two middle points\n",
        "    return (b+c)/2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "id": "1KvvfkE_QpEu"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_step(phi, data,  model):\n",
        "  data_x, data_y = data\n",
        "\n",
        "  # Compute the gradient\n",
        "  grad = compute_gradient(data_x, data_y, phi)\n",
        "\n",
        "  # Line search to find best step size (assume negative direction)\n",
        "  alpha = line_search(data, model, phi, -grad)\n",
        "\n",
        "  # Update parameters\n",
        "  phi = phi + alpha * (-grad)\n",
        "\n",
        "  return phi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGv9KewtQpEu"
      },
      "outputs": [],
      "source": [
        "# Initialize the parameters and draw the model\n",
        "n_steps = 10\n",
        "phi_all = np.zeros((2,n_steps+1))\n",
        "phi_all[0,0] = 1.6\n",
        "phi_all[1,0] = -0.5\n",
        "\n",
        "# Measure loss and draw initial model\n",
        "loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,0:1])\n",
        "draw_model(data,model,phi_all[:,0:1], \"Initial parameters, Loss = %f\"%(loss))\n",
        "\n",
        "# Repeatedly take gradient descent steps\n",
        "for c_step in range (n_steps):\n",
        "  # Do gradient descent step\n",
        "  phi_all[:,c_step+1:c_step+2] = gradient_descent_step(phi_all[:,c_step:c_step+1],data, model)\n",
        "  # Measure loss and draw model\n",
        "  loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,c_step+1:c_step+2])\n",
        "  draw_model(data,model,phi_all[:,c_step+1], \"Iteration %d, loss = %f\"%(c_step+1,loss))\n",
        "\n",
        "# Draw the trajectory on the loss function\n",
        "draw_loss_function(compute_loss, data, model,phi_all)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "309WvDEzQtPC"
      },
      "source": [
        "# **Stochastic gradient descent**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "id": "YJths4HmQtPD"
      },
      "outputs": [],
      "source": [
        "# Training data of 30 pairs {x_i, y_i}\n",
        "# We'll try to fit the Gabor model to these data\n",
        "data = np.array([[-1.920e+00,-1.422e+01,1.490e+00,-1.940e+00,-2.389e+00,-5.090e+00,\n",
        "                 -8.861e+00,3.578e+00,-6.010e+00,-6.995e+00,3.634e+00,8.743e-01,\n",
        "                 -1.096e+01,4.073e-01,-9.467e+00,8.560e+00,1.062e+01,-1.729e-01,\n",
        "                  1.040e+01,-1.261e+01,1.574e-01,-1.304e+01,-2.156e+00,-1.210e+01,\n",
        "                 -1.119e+01,2.902e+00,-8.220e+00,-1.179e+01,-8.391e+00,-4.505e+00],\n",
        "                  [-1.051e+00,-2.482e-02,8.896e-01,-4.943e-01,-9.371e-01,4.306e-01,\n",
        "                  9.577e-03,-7.944e-02 ,1.624e-01,-2.682e-01,-3.129e-01,8.303e-01,\n",
        "                  -2.365e-02,5.098e-01,-2.777e-01,3.367e-01,1.927e-01,-2.222e-01,\n",
        "                  6.352e-02,6.888e-03,3.224e-02,1.091e-02,-5.706e-01,-5.258e-02,\n",
        "                  -3.666e-02,1.709e-01,-4.805e-02,2.008e-01,-1.904e-01,5.952e-01]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "UdoQ9matQtPD"
      },
      "outputs": [],
      "source": [
        "# Model Def\n",
        "def model(phi,x):\n",
        "  sin_component = np.sin(phi[0] + 0.06 * phi[1] * x)\n",
        "  gauss_component = np.exp(-(phi[0] + 0.06 * phi[1] * x) * (phi[0] + 0.06 * phi[1] * x) / 32)\n",
        "  y_pred= sin_component * gauss_component\n",
        "  return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "id": "cT9lOqtnQtPD"
      },
      "outputs": [],
      "source": [
        "# Draw model\n",
        "def draw_model(data,model,phi,title=None):\n",
        "  x_model = np.arange(-15,15,0.1)\n",
        "  y_model = model(phi,x_model)\n",
        "\n",
        "  fix, ax = plt.subplots()\n",
        "  ax.plot(data[0,:],data[1,:],'bo')\n",
        "  ax.plot(x_model,y_model,'m-')\n",
        "  ax.set_xlim([-15,15]);ax.set_ylim([-1,1])\n",
        "  ax.set_xlabel('x'); ax.set_ylabel('y')\n",
        "  if title is not None:\n",
        "    ax.set_title(title)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77IsUuRcQtPD"
      },
      "outputs": [],
      "source": [
        "# Initialize the parameters and draw the model\n",
        "phi = np.zeros((2,1))\n",
        "phi[0] =  -7    # Horizontal offset\n",
        "phi[1] =  20     # Frequency\n",
        "draw_model(data,model,phi, \"Initial parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dol3vDpmQtPD"
      },
      "source": [
        "Now let's compute the sum of squares loss for the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "id": "lcpFSd--QtPD"
      },
      "outputs": [],
      "source": [
        "def compute_loss(data_x, data_y, model, phi):\n",
        "  # Make predictions\n",
        "  pred_y = model(phi, data_x)\n",
        "\n",
        "  # Compute squared differences\n",
        "  squared_diffs = (pred_y - data_y) ** 2\n",
        "\n",
        "  # Sum and return the loss\n",
        "  loss = np.sum(squared_diffs)\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJV8JvlsQtPE"
      },
      "source": [
        "Plot the whole loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "id": "ctvJZqvrQtPE"
      },
      "outputs": [],
      "source": [
        "def draw_loss_function(compute_loss, data,  model, phi_iters = None):\n",
        "  # Define pretty colormap\n",
        "  my_colormap_vals_hex =('2a0902', '2b0a03', '2c0b04', '2d0c05', '2e0c06', '2f0d07', '300d08', '310e09', '320f0a', '330f0b', '34100b', '35110c', '36110d', '37120e', '38120f', '39130f', '3a1410', '3b1411', '3c1511', '3d1612', '3e1613', '3f1713', '401714', '411814', '421915', '431915', '451a16', '461b16', '471b17', '481c17', '491d18', '4a1d18', '4b1e19', '4c1f19', '4d1f1a', '4e201b', '50211b', '51211c', '52221c', '53231d', '54231d', '55241e', '56251e', '57261f', '58261f', '592720', '5b2821', '5c2821', '5d2922', '5e2a22', '5f2b23', '602b23', '612c24', '622d25', '632e25', '652e26', '662f26', '673027', '683027', '693128', '6a3229', '6b3329', '6c342a', '6d342a', '6f352b', '70362c', '71372c', '72372d', '73382e', '74392e', '753a2f', '763a2f', '773b30', '783c31', '7a3d31', '7b3e32', '7c3e33', '7d3f33', '7e4034', '7f4134', '804235', '814236', '824336', '834437', '854538', '864638', '874739', '88473a', '89483a', '8a493b', '8b4a3c', '8c4b3c', '8d4c3d', '8e4c3e', '8f4d3f', '904e3f', '924f40', '935041', '945141', '955242', '965343', '975343', '985444', '995545', '9a5646', '9b5746', '9c5847', '9d5948', '9e5a49', '9f5a49', 'a05b4a', 'a15c4b', 'a35d4b', 'a45e4c', 'a55f4d', 'a6604e', 'a7614e', 'a8624f', 'a96350', 'aa6451', 'ab6552', 'ac6552', 'ad6653', 'ae6754', 'af6855', 'b06955', 'b16a56', 'b26b57', 'b36c58', 'b46d59', 'b56e59', 'b66f5a', 'b7705b', 'b8715c', 'b9725d', 'ba735d', 'bb745e', 'bc755f', 'bd7660', 'be7761', 'bf7862', 'c07962', 'c17a63', 'c27b64', 'c27c65', 'c37d66', 'c47e67', 'c57f68', 'c68068', 'c78169', 'c8826a', 'c9836b', 'ca846c', 'cb856d', 'cc866e', 'cd876f', 'ce886f', 'ce8970', 'cf8a71', 'd08b72', 'd18c73', 'd28d74', 'd38e75', 'd48f76', 'd59077', 'd59178', 'd69279', 'd7937a', 'd8957b', 'd9967b', 'da977c', 'da987d', 'db997e', 'dc9a7f', 'dd9b80', 'de9c81', 'de9d82', 'df9e83', 'e09f84', 'e1a185', 'e2a286', 'e2a387', 'e3a488', 'e4a589', 'e5a68a', 'e5a78b', 'e6a88c', 'e7aa8d', 'e7ab8e', 'e8ac8f', 'e9ad90', 'eaae91', 'eaaf92', 'ebb093', 'ecb295', 'ecb396', 'edb497', 'eeb598', 'eeb699', 'efb79a', 'efb99b', 'f0ba9c', 'f1bb9d', 'f1bc9e', 'f2bd9f', 'f2bfa1', 'f3c0a2', 'f3c1a3', 'f4c2a4', 'f5c3a5', 'f5c5a6', 'f6c6a7', 'f6c7a8', 'f7c8aa', 'f7c9ab', 'f8cbac', 'f8ccad', 'f8cdae', 'f9ceb0', 'f9d0b1', 'fad1b2', 'fad2b3', 'fbd3b4', 'fbd5b6', 'fbd6b7', 'fcd7b8', 'fcd8b9', 'fcdaba', 'fddbbc', 'fddcbd', 'fddebe', 'fddfbf', 'fee0c1', 'fee1c2', 'fee3c3', 'fee4c5', 'ffe5c6', 'ffe7c7', 'ffe8c9', 'ffe9ca', 'ffebcb', 'ffeccd', 'ffedce', 'ffefcf', 'fff0d1', 'fff2d2', 'fff3d3', 'fff4d5', 'fff6d6', 'fff7d8', 'fff8d9', 'fffada', 'fffbdc', 'fffcdd', 'fffedf', 'ffffe0')\n",
        "  my_colormap_vals_dec = np.array([int(element,base=16) for element in my_colormap_vals_hex])\n",
        "  r = np.floor(my_colormap_vals_dec/(256*256))\n",
        "  g = np.floor((my_colormap_vals_dec - r *256 *256)/256)\n",
        "  b = np.floor(my_colormap_vals_dec - r * 256 *256 - g * 256)\n",
        "  my_colormap = ListedColormap(np.vstack((r,g,b)).transpose()/255.0)\n",
        "\n",
        "  # Make grid of offset/frequency values to plot\n",
        "  offsets_mesh, freqs_mesh = np.meshgrid(np.arange(-10,10.0,0.1), np.arange(2.5,22.5,0.1))\n",
        "  loss_mesh = np.zeros_like(freqs_mesh)\n",
        "  # Compute loss for every set of parameters\n",
        "  for idslope, slope in np.ndenumerate(freqs_mesh):\n",
        "     loss_mesh[idslope] = compute_loss(data[0,:], data[1,:], model, np.array([[offsets_mesh[idslope]], [slope]]))\n",
        "\n",
        "  fig,ax = plt.subplots()\n",
        "  fig.set_size_inches(8,8)\n",
        "  ax.contourf(offsets_mesh,freqs_mesh,loss_mesh,256,cmap=my_colormap)\n",
        "  ax.contour(offsets_mesh,freqs_mesh,loss_mesh,20,colors=['#80808080'])\n",
        "  if phi_iters is not None:\n",
        "    ax.plot(phi_iters[0,:], phi_iters[1,:],'go-')\n",
        "  ax.set_ylim([2.5,22.5])\n",
        "  ax.set_xlabel('Offset $\\phi_{0}$'); ax.set_ylabel('Frequency, $\\phi_{1}$')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoSws-H-QtPE"
      },
      "outputs": [],
      "source": [
        "draw_loss_function(compute_loss, data, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixsfKMZ9QtPF"
      },
      "source": [
        "Now let's compute the gradient vector for a given set of parameters:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial L}{\\partial \\boldsymbol\\phi} = \\begin{bmatrix}\\frac{\\partial L}{\\partial \\phi_0} \\\\\\frac{\\partial L}{\\partial \\phi_1} \\end{bmatrix}.\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "kpspdI3KQtPF"
      },
      "outputs": [],
      "source": [
        "def gabor_deriv_phi0(data_x,data_y,phi0, phi1):\n",
        "    x = 0.06 * phi1 * data_x + phi0\n",
        "    y = data_y\n",
        "    cos_component = np.cos(x)\n",
        "    sin_component = np.sin(x)\n",
        "    gauss_component = np.exp(-0.5 * x *x / 16)\n",
        "    deriv = cos_component * gauss_component - sin_component * gauss_component * x / 16\n",
        "    deriv = 2* deriv * (sin_component * gauss_component - y)\n",
        "    return np.sum(deriv)\n",
        "\n",
        "def gabor_deriv_phi1(data_x, data_y,phi0, phi1):\n",
        "    x = 0.06 * phi1 * data_x + phi0\n",
        "    y = data_y\n",
        "    cos_component = np.cos(x)\n",
        "    sin_component = np.sin(x)\n",
        "    gauss_component = np.exp(-0.5 * x *x / 16)\n",
        "    deriv = 0.06 * data_x * cos_component * gauss_component - 0.06 * data_x*sin_component * gauss_component * x / 16\n",
        "    deriv = 2*deriv * (sin_component * gauss_component - y)\n",
        "    return np.sum(deriv)\n",
        "\n",
        "def compute_gradient(data_x, data_y, phi):\n",
        "    dl_dphi0 = gabor_deriv_phi0(data_x, data_y, phi[0],phi[1])\n",
        "    dl_dphi1 = gabor_deriv_phi1(data_x, data_y, phi[0],phi[1])\n",
        "    # Return the gradient\n",
        "    return np.array([[dl_dphi0],[dl_dphi1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lhojynz7QtPF"
      },
      "outputs": [],
      "source": [
        "# Compute the gradient using your function\n",
        "gradient = compute_gradient(data[0,:],data[1,:], phi)\n",
        "print(\"Your gradients: (%3.3f,%3.3f)\"%(gradient[0],gradient[1]))\n",
        "# Approximate the gradients with finite differences\n",
        "delta = 0.0001\n",
        "dl_dphi0_est = (compute_loss(data[0,:],data[1,:],model,phi+np.array([[delta],[0]])) - \\\n",
        "                    compute_loss(data[0,:],data[1,:],model,phi))/delta\n",
        "dl_dphi1_est = (compute_loss(data[0,:],data[1,:],model,phi+np.array([[0],[delta]])) - \\\n",
        "                    compute_loss(data[0,:],data[1,:],model,phi))/delta\n",
        "print(\"Approx gradients: (%3.3f,%3.3f)\"%(dl_dphi0_est,dl_dphi1_est))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "id": "4l-ueLk-oAxV"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_step_fixed_learning_rate(phi, data, alpha):\n",
        "  #take a fixed size step of size alpha without using line search\n",
        "  data_x, data_y = data\n",
        "\n",
        "  # Compute the gradient\n",
        "  grad = compute_gradient(data_x, data_y, phi)\n",
        "\n",
        "  # Update phi with fixed step size\n",
        "  phi = phi - alpha * grad\n",
        "\n",
        "  return phi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi9MX_GRpM41"
      },
      "outputs": [],
      "source": [
        "# Initialize the parameters\n",
        "n_steps = 21\n",
        "phi_all = np.zeros((2,n_steps+1))\n",
        "phi_all[0,0] = -1.5\n",
        "phi_all[1,0] = 8.5\n",
        "\n",
        "# Measure loss and draw initial model\n",
        "loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,0:1])\n",
        "draw_model(data,model,phi_all[:,0:1], \"Initial parameters, Loss = %f\"%(loss))\n",
        "\n",
        "for c_step in range (n_steps):\n",
        "  # Do gradient descent step\n",
        "  phi_all[:,c_step+1:c_step+2] = gradient_descent_step_fixed_learning_rate(phi_all[:,c_step:c_step+1],data, alpha =0.2)\n",
        "  # Measure loss and draw model every 8th step\n",
        "  if c_step % 8 == 0:\n",
        "    loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,c_step+1:c_step+2])\n",
        "    draw_model(data,model,phi_all[:,c_step+1], \"Iteration %d, loss = %f\"%(c_step+1,loss))\n",
        "\n",
        "draw_loss_function(compute_loss, data, model,phi_all)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In6sQ5YCpMqn"
      },
      "outputs": [],
      "source": [
        "# What happens if you set it too large?\n",
        "# Initialize the parameters\n",
        "n_steps = 21\n",
        "phi_all = np.zeros((2,n_steps+1))\n",
        "phi_all[0,0] = -1.5\n",
        "phi_all[1,0] = 8.5\n",
        "\n",
        "# Measure loss and draw initial model\n",
        "loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,0:1])\n",
        "\n",
        "for c_step in range (n_steps):\n",
        "  # Do gradient descent step\n",
        "  phi_all[:,c_step+1:c_step+2] = gradient_descent_step_fixed_learning_rate(phi_all[:,c_step:c_step+1],data, alpha =0.5)\n",
        "  # Measure loss and draw model every 8th step\n",
        "  if c_step % 8 == 0:\n",
        "    loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,c_step+1:c_step+2])\n",
        "\n",
        "\n",
        "\n",
        "draw_loss_function(compute_loss, data, model,phi_all)\n",
        "draw_model(data,model,phi_all[:,c_step+1], \"Iteration %d, loss = %f\"%(c_step+1,loss))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What happens if you set it too small?\n",
        "# Initialize the parameters\n",
        "n_steps = 21\n",
        "phi_all = np.zeros((2,n_steps+1))\n",
        "phi_all[0,0] = -1.5\n",
        "phi_all[1,0] = 8.5\n",
        "\n",
        "# Measure loss and draw initial model\n",
        "loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,0:1])\n",
        "\n",
        "for c_step in range (n_steps):\n",
        "  # Do gradient descent step\n",
        "  phi_all[:,c_step+1:c_step+2] = gradient_descent_step_fixed_learning_rate(phi_all[:,c_step:c_step+1],data, alpha =0.1)\n",
        "  # Measure loss and draw model every 4th step\n",
        "  if c_step % 4 == 0:\n",
        "    loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,c_step+1:c_step+2])\n",
        "\n",
        "\n",
        "\n",
        "draw_loss_function(compute_loss, data, model,phi_all)\n",
        "draw_model(data,model,phi_all[:,c_step+1], \"Iteration %d, loss = %f\"%(c_step+1,loss))"
      ],
      "metadata": {
        "id": "yy4ynmzjhcXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "id": "VKTC9-1Gpm3N"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent_step(phi, data, alpha, batch_size):\n",
        "  # Fixed size step of size alpha but only using a subset (batch) of the data at each step\n",
        "  # Can use np.random.permutation to generate a random permutation of the n_data = data.shape[1] indices\n",
        "  data_x, data_y = data\n",
        "  n_data = data_x.shape[0]\n",
        "\n",
        "  # Random permutation of indices\n",
        "  indices = np.random.permutation(n_data)[:batch_size]\n",
        "\n",
        "  # Select the batch\n",
        "  batch_x = data_x[indices]\n",
        "  batch_y = data_y[indices]\n",
        "\n",
        "  # Compute the gradient from the batch\n",
        "  grad = compute_gradient(batch_x, batch_y, phi)\n",
        "\n",
        "  # Update phi\n",
        "  phi = phi - alpha * grad\n",
        "\n",
        "\n",
        "  return phi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "469OP_UHskJ4"
      },
      "outputs": [],
      "source": [
        "# Initialize the parameters\n",
        "np.random.seed(1)\n",
        "n_steps = 41\n",
        "phi_all = np.zeros((2,n_steps+1))\n",
        "phi_all[0,0] = 3.5\n",
        "phi_all[1,0] = 6.5\n",
        "\n",
        "# Measure loss and draw initial model\n",
        "loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,0:1])\n",
        "draw_model(data,model,phi_all[:,0:1], \"Initial parameters, Loss = %f\"%(loss))\n",
        "\n",
        "for c_step in range (n_steps):\n",
        "  # Do gradient descent step\n",
        "  phi_all[:,c_step+1:c_step+2] = stochastic_gradient_descent_step(phi_all[:,c_step:c_step+1],data, alpha =0.8, batch_size=5)\n",
        "  # Measure loss and draw model every 8th step\n",
        "  if c_step % 8 == 0:\n",
        "    loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,c_step+1:c_step+2])\n",
        "    draw_model(data,model,phi_all[:,c_step+1], \"Iteration %d, loss = %f\"%(c_step+1,loss))\n",
        "\n",
        "draw_loss_function(compute_loss, data, model,phi_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxE2kTa3s29p"
      },
      "outputs": [],
      "source": [
        "#Different learning rates, starting points, batch sizes, number of steps will get you different results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Adam**"
      ],
      "metadata": {
        "id": "ysg9OHZq07YC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function that we wish to find the minimum of (normally would be defined implicitly by data and loss)\n",
        "def loss(phi0, phi1):\n",
        "    height = np.exp(-0.5 * (phi1 * phi1)*4.0)\n",
        "    height = height * np. exp(-0.5* (phi0-0.7) *(phi0-0.7)/4.0)\n",
        "    return 1.0-height\n",
        "\n",
        "# Compute the gradients of this function (used finite differences)\n",
        "def get_loss_gradient(phi0, phi1):\n",
        "    delta_phi = 0.00001;\n",
        "    gradient = np.zeros((2,1));\n",
        "    gradient[0] = (loss(phi0+delta_phi/2.0, phi1) - loss(phi0-delta_phi/2.0, phi1))/delta_phi\n",
        "    gradient[1] = (loss(phi0, phi1+delta_phi/2.0) - loss(phi0, phi1-delta_phi/2.0))/delta_phi\n",
        "    return gradient[:,0];\n",
        "\n",
        "# Compute the loss function at a range of values of phi0 and phi1 for plotting\n",
        "def get_loss_function_for_plot():\n",
        "  grid_values = np.arange(-1.0,1.0,0.01);\n",
        "  phi0mesh, phi1mesh = np.meshgrid(grid_values, grid_values)\n",
        "  loss_function = np.zeros((grid_values.size, grid_values.size))\n",
        "  for idphi0, phi0 in enumerate(grid_values):\n",
        "      for idphi1, phi1 in enumerate(grid_values):\n",
        "          loss_function[idphi0, idphi1] = loss(phi1,phi0)\n",
        "  return loss_function, phi0mesh, phi1mesh"
      ],
      "metadata": {
        "id": "GTrgOKhp16zw"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fancy colormap\n",
        "my_colormap_vals_hex =('2a0902', '2b0a03', '2c0b04', '2d0c05', '2e0c06', '2f0d07', '300d08', '310e09', '320f0a', '330f0b', '34100b', '35110c', '36110d', '37120e', '38120f', '39130f', '3a1410', '3b1411', '3c1511', '3d1612', '3e1613', '3f1713', '401714', '411814', '421915', '431915', '451a16', '461b16', '471b17', '481c17', '491d18', '4a1d18', '4b1e19', '4c1f19', '4d1f1a', '4e201b', '50211b', '51211c', '52221c', '53231d', '54231d', '55241e', '56251e', '57261f', '58261f', '592720', '5b2821', '5c2821', '5d2922', '5e2a22', '5f2b23', '602b23', '612c24', '622d25', '632e25', '652e26', '662f26', '673027', '683027', '693128', '6a3229', '6b3329', '6c342a', '6d342a', '6f352b', '70362c', '71372c', '72372d', '73382e', '74392e', '753a2f', '763a2f', '773b30', '783c31', '7a3d31', '7b3e32', '7c3e33', '7d3f33', '7e4034', '7f4134', '804235', '814236', '824336', '834437', '854538', '864638', '874739', '88473a', '89483a', '8a493b', '8b4a3c', '8c4b3c', '8d4c3d', '8e4c3e', '8f4d3f', '904e3f', '924f40', '935041', '945141', '955242', '965343', '975343', '985444', '995545', '9a5646', '9b5746', '9c5847', '9d5948', '9e5a49', '9f5a49', 'a05b4a', 'a15c4b', 'a35d4b', 'a45e4c', 'a55f4d', 'a6604e', 'a7614e', 'a8624f', 'a96350', 'aa6451', 'ab6552', 'ac6552', 'ad6653', 'ae6754', 'af6855', 'b06955', 'b16a56', 'b26b57', 'b36c58', 'b46d59', 'b56e59', 'b66f5a', 'b7705b', 'b8715c', 'b9725d', 'ba735d', 'bb745e', 'bc755f', 'bd7660', 'be7761', 'bf7862', 'c07962', 'c17a63', 'c27b64', 'c27c65', 'c37d66', 'c47e67', 'c57f68', 'c68068', 'c78169', 'c8826a', 'c9836b', 'ca846c', 'cb856d', 'cc866e', 'cd876f', 'ce886f', 'ce8970', 'cf8a71', 'd08b72', 'd18c73', 'd28d74', 'd38e75', 'd48f76', 'd59077', 'd59178', 'd69279', 'd7937a', 'd8957b', 'd9967b', 'da977c', 'da987d', 'db997e', 'dc9a7f', 'dd9b80', 'de9c81', 'de9d82', 'df9e83', 'e09f84', 'e1a185', 'e2a286', 'e2a387', 'e3a488', 'e4a589', 'e5a68a', 'e5a78b', 'e6a88c', 'e7aa8d', 'e7ab8e', 'e8ac8f', 'e9ad90', 'eaae91', 'eaaf92', 'ebb093', 'ecb295', 'ecb396', 'edb497', 'eeb598', 'eeb699', 'efb79a', 'efb99b', 'f0ba9c', 'f1bb9d', 'f1bc9e', 'f2bd9f', 'f2bfa1', 'f3c0a2', 'f3c1a3', 'f4c2a4', 'f5c3a5', 'f5c5a6', 'f6c6a7', 'f6c7a8', 'f7c8aa', 'f7c9ab', 'f8cbac', 'f8ccad', 'f8cdae', 'f9ceb0', 'f9d0b1', 'fad1b2', 'fad2b3', 'fbd3b4', 'fbd5b6', 'fbd6b7', 'fcd7b8', 'fcd8b9', 'fcdaba', 'fddbbc', 'fddcbd', 'fddebe', 'fddfbf', 'fee0c1', 'fee1c2', 'fee3c3', 'fee4c5', 'ffe5c6', 'ffe7c7', 'ffe8c9', 'ffe9ca', 'ffebcb', 'ffeccd', 'ffedce', 'ffefcf', 'fff0d1', 'fff2d2', 'fff3d3', 'fff4d5', 'fff6d6', 'fff7d8', 'fff8d9', 'fffada', 'fffbdc', 'fffcdd', 'fffedf', 'ffffe0')\n",
        "my_colormap_vals_dec = np.array([int(element,base=16) for element in my_colormap_vals_hex])\n",
        "r = np.floor(my_colormap_vals_dec/(256*256))\n",
        "g = np.floor((my_colormap_vals_dec - r *256 *256)/256)\n",
        "b = np.floor(my_colormap_vals_dec - r * 256 *256 - g * 256)\n",
        "my_colormap_vals = np.vstack((r,g,b)).transpose()/255.0\n",
        "my_colormap = ListedColormap(my_colormap_vals)\n",
        "\n",
        "# Plotting function\n",
        "def draw_function(phi0mesh, phi1mesh, loss_function, my_colormap, opt_path):\n",
        "    fig = plt.figure();\n",
        "    ax = plt.axes();\n",
        "    fig.set_size_inches(7,7)\n",
        "    ax.contourf(phi0mesh, phi1mesh, loss_function, 256, cmap=my_colormap);\n",
        "    ax.contour(phi0mesh, phi1mesh, loss_function, 20, colors=['#80808080'])\n",
        "    ax.plot(opt_path[0,:], opt_path[1,:],'-', color='#a0d9d3ff')\n",
        "    ax.plot(opt_path[0,:], opt_path[1,:],'.', color='#a0d9d3ff',markersize=10)\n",
        "    ax.set_xlabel(r\"$\\phi_{0}$\")\n",
        "    ax.set_ylabel(r\"$\\phi_{1}$\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "YKijFyuH4ZJD"
      },
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple fixed step size gradient descent\n",
        "def grad_descent(start_posn, n_steps, alpha):\n",
        "    grad_path = np.zeros((2, n_steps+1));\n",
        "    grad_path[:,0] = start_posn[:,0];\n",
        "    for c_step in range(n_steps):\n",
        "        this_grad = get_loss_gradient(grad_path[0,c_step], grad_path[1,c_step]);\n",
        "        grad_path[:,c_step+1] = grad_path[:,c_step] - alpha * this_grad\n",
        "    return grad_path;"
      ],
      "metadata": {
        "id": "Afxr7RqR8s7Q"
      },
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by running gradient descent with a fixed step size for this loss function."
      ],
      "metadata": {
        "id": "MXZL8lu3-EUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function, phi0mesh, phi1mesh = get_loss_function_for_plot() ;\n",
        "\n",
        "start_posn = np.zeros((2,1));\n",
        "start_posn[0,0] = -0.7; start_posn[1,0] = -0.9\n",
        "\n",
        "# Run gradient descent\n",
        "grad_path1 = grad_descent(start_posn, n_steps=200, alpha = 0.08)\n",
        "draw_function(phi0mesh, phi1mesh, loss_function, my_colormap, grad_path1)\n",
        "grad_path2 = grad_descent(start_posn, n_steps=40, alpha= 1.0)\n",
        "draw_function(phi0mesh, phi1mesh, loss_function, my_colormap, grad_path2)"
      ],
      "metadata": {
        "id": "fgkwVEal8stH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the function changes much faster in $\\phi_1$ than in $\\phi_0$, there is no great step size to choose.  If we set the step size so that it makes sensible progress in the $\\phi_1$ direction, then it takes many iterations to converge.  If we set the step size so that we make sensible progress in the $\\phi_0$ direction, then the path oscillates in the $\\phi_1$ direction.  \n",
        "\n",
        "With Adam we determine which way is downhill along each axis and move a fixed distance in that direction."
      ],
      "metadata": {
        "id": "AN2uNxaa-bRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalized_gradients(start_posn, n_steps, alpha,  epsilon=1e-20):\n",
        "    grad_path = np.zeros((2, n_steps+1));\n",
        "    grad_path[:,0] = start_posn[:,0];\n",
        "    for c_step in range(n_steps):\n",
        "        # Measure the gradient\n",
        "        m = get_loss_gradient(grad_path[0,c_step], grad_path[1,c_step]);\n",
        "        #Compute the squared gradient\n",
        "        v = m**2\n",
        "\n",
        "        # Apply the update rule (equation 6.14)\n",
        "        grad_path[:,c_step+1] = grad_path[:,c_step] - alpha * (m / (np.sqrt(v) + epsilon))\n",
        "\n",
        "    return grad_path;"
      ],
      "metadata": {
        "id": "IqX2zP_29gLF"
      },
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try out normalized gradients\n",
        "start_posn = np.zeros((2,1));\n",
        "start_posn[0,0] = -0.7; start_posn[1,0] = -0.9\n",
        "\n",
        "# Run gradient descent\n",
        "grad_path1 = normalized_gradients(start_posn, n_steps=40, alpha = 0.08)\n",
        "draw_function(phi0mesh, phi1mesh, loss_function, my_colormap, grad_path1)"
      ],
      "metadata": {
        "id": "wxe-dKW5Chv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This moves towards the minimum at a sensible speed, but we never actually converge -- the solution just bounces back and forth between the last two points.  To make it converge, we add momentum to both the estimates of the gradient and the pointwise squared gradient.  We also modify the statistics by a factor that depends on the time to make sure the progress is not slow to start with."
      ],
      "metadata": {
        "id": "_6KoKBJdGGI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(start_posn, n_steps, alpha,  beta=0.9, gamma=0.99, epsilon=1e-20):\n",
        "    grad_path = np.zeros((2, n_steps+1));\n",
        "    grad_path[:,0] = start_posn[:,0];\n",
        "    m = np.zeros_like(grad_path[:,0])\n",
        "    v = np.zeros_like(grad_path[:,0])\n",
        "    for c_step in range(n_steps):\n",
        "        # Measure the gradient\n",
        "        grad = get_loss_gradient(grad_path[0,c_step], grad_path[1,c_step])\n",
        "        #Update the momentum based gradient estimate\n",
        "        m = beta * m + (1 - beta) * grad\n",
        "\n",
        "\n",
        "        # Update the momentum based squared gradient estimate\n",
        "        v = gamma * v + (1 - gamma) * (grad**2)\n",
        "\n",
        "        #Modify the statistics (equation 6.16)\n",
        "        m_tilde = m / (1 - np.power(beta, c_step + 1))\n",
        "        v_tilde = v / (1 - np.power(gamma, c_step + 1))\n",
        "\n",
        "\n",
        "        # Apply the update rule (equation 6.17)\n",
        "        grad_path[:,c_step+1] = grad_path[:,c_step] - alpha * (m_tilde / (np.sqrt(v_tilde) + epsilon))\n",
        "\n",
        "    return grad_path;"
      ],
      "metadata": {
        "id": "BKUhZSGgDEm0"
      },
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_posn = np.zeros((2,1));\n",
        "start_posn[0,0] = -0.7; start_posn[1,0] = -0.9\n",
        "\n",
        "# Run gradient descent\n",
        "grad_path1 = adam(start_posn, n_steps=60, alpha = 0.05)\n",
        "draw_function(phi0mesh, phi1mesh, loss_function, my_colormap, grad_path1)"
      ],
      "metadata": {
        "id": "sg5X18P3IbYo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}